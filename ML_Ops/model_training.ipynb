{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52df852",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Crop Image Classification Pipeline\n",
    "\n",
    "This script implements a complete pipeline for training, validating and testing\n",
    "image classification models on crop disease datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Import Libraries =====\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import shutil\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm  # For progress bars\n",
    "import wandb  # For experiment tracking\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"training.log\"), logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts, OneCycleLR\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Advanced augmentation\n",
    "try:\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    ALBUMENTATIONS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Albumentations not available. Install with: pip install albumentations\")\n",
    "    ALBUMENTATIONS_AVAILABLE = False\n",
    "\n",
    "# scikit-learn for evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# For visualization\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# Mixed precision training\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0609c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility across all libraries\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "set_seed()\n",
    "\n",
    "\n",
    "# ===== Section: Enhanced Configuration Settings =====\n",
    "class Config:\n",
    "    \"\"\"Enhanced configuration with checkpointing and optimization features\"\"\"\n",
    "    # Paths\n",
    "    data_dir = \"./dataset\"\n",
    "    output_dir = \"./models\"\n",
    "    results_dir = \"./results\"\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    tensorboard_dir = \"./runs\"\n",
    "\n",
    "    # Dataset settings\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    use_weighted_sampling = True  # For imbalanced datasets\n",
    "\n",
    "    # Model settings\n",
    "    model_type = \"resnet101\"  # Options: 'custom_cnn', 'resnet18', 'resnet50', 'resnet101', 'efficientnet_b0'\n",
    "    pretrained = True\n",
    "    freeze_backbone = False  # Whether to freeze backbone initially\n",
    "    unfreeze_epoch = 10  # Epoch to unfreeze backbone if frozen\n",
    "\n",
    "    # Training settings\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    gradient_clip_val = 1.0\n",
    "\n",
    "    # Advanced training options\n",
    "    use_mixed_precision = True  # Automatic Mixed Precision\n",
    "    use_cosine_annealing = True  # Cosine annealing scheduler\n",
    "    warmup_epochs = 5\n",
    "    label_smoothing = 0.1\n",
    "\n",
    "    # Early stopping and checkpointing\n",
    "    patience = 10\n",
    "    save_top_k = 3  # Save top k models\n",
    "    monitor_metric = 'val_acc'  # Metric to monitor for checkpointing\n",
    "    checkpoint_every = 5  # Save checkpoint every N epochs\n",
    "\n",
    "    # Data augmentation\n",
    "    use_advanced_augmentation = True\n",
    "    augmentation_strength = 'medium'  # 'light', 'medium', 'heavy'\n",
    "\n",
    "    # Image settings\n",
    "    img_size = 224\n",
    "    normalize_mean = [0.485, 0.456, 0.406]\n",
    "    normalize_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Device settings\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_workers = 4\n",
    "    pin_memory = True\n",
    "\n",
    "    # Experiment tracking\n",
    "    use_wandb = False  # Set to True to use Weights & Biases\n",
    "    use_tensorboard = True\n",
    "    experiment_name = f\"crop_disease_{model_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    # Resume training\n",
    "    resume_from_checkpoint = None  # Path to checkpoint to resume from\n",
    "\n",
    "\n",
    "# Create output directories\n",
    "for directory in [Config.output_dir, Config.results_dir, Config.checkpoint_dir, Config.tensorboard_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Initialize experiment tracking\n",
    "if Config.use_wandb:\n",
    "    try:\n",
    "        wandb.init(\n",
    "            project=\"crop-disease-classification\",\n",
    "            name=Config.experiment_name,\n",
    "            config=vars(Config)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize wandb: {e}\")\n",
    "        Config.use_wandb = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3a25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Enhanced Data Preparation =====\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    \"\"\"Wrapper for Albumentations transforms\"\"\"\n",
    "    \n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convert PIL to numpy\n",
    "        img_np = np.array(img)\n",
    "        # Apply transform\n",
    "        augmented = self.transform(image=img_np)\n",
    "        return augmented['image']\n",
    "\n",
    "\n",
    "def get_advanced_transforms():\n",
    "    \"\"\"Get advanced data transformations using Albumentations\"\"\"\n",
    "    if not ALBUMENTATIONS_AVAILABLE:\n",
    "        return get_data_transforms()  # Fallback to basic transforms\n",
    "    \n",
    "    # Training transforms with different strength levels\n",
    "    if Config.augmentation_strength == 'light':\n",
    "        train_transform = A.Compose([\n",
    "            A.Resize(Config.img_size + 32, Config.img_size + 32),\n",
    "            A.RandomCrop(Config.img_size, Config.img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.5),\n",
    "            A.Normalize(Config.normalize_mean, Config.normalize_std),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    elif Config.augmentation_strength == 'medium':\n",
    "        train_transform = A.Compose([\n",
    "            A.Resize(Config.img_size + 32, Config.img_size + 32),\n",
    "            A.RandomCrop(Config.img_size, Config.img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.Rotate(limit=20, p=0.7),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.7),\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(1, 3), p=1.0),\n",
    "                A.MotionBlur(blur_limit=3, p=1.0),\n",
    "            ], p=0.3),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "                A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=1.0),\n",
    "            ], p=0.5),\n",
    "            A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, p=0.3),\n",
    "            A.Normalize(Config.normalize_mean, Config.normalize_std),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:  # heavy\n",
    "        train_transform = A.Compose([\n",
    "            A.Resize(Config.img_size + 64, Config.img_size + 64),\n",
    "            A.RandomCrop(Config.img_size, Config.img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.Rotate(limit=30, p=0.8),\n",
    "            A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),\n",
    "                A.GridDistortion(p=1.0),\n",
    "                A.OpticalDistortion(distort_limit=0.1, shift_limit=0.1, p=1.0),\n",
    "            ], p=0.3),\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(1, 5), p=1.0),\n",
    "                A.MotionBlur(blur_limit=5, p=1.0),\n",
    "            ], p=0.5),\n",
    "            A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2, p=0.8),\n",
    "            A.Cutout(num_holes=16, max_h_size=16, max_w_size=16, fill_value=0, p=0.5),\n",
    "            A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "            A.Normalize(Config.normalize_mean, Config.normalize_std),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    # Validation/Test transforms\n",
    "    val_test_transform = A.Compose([\n",
    "        A.Resize(Config.img_size + 32, Config.img_size + 32),\n",
    "        A.CenterCrop(Config.img_size, Config.img_size),\n",
    "        A.Normalize(Config.normalize_mean, Config.normalize_std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    return AlbumentationsTransform(train_transform), AlbumentationsTransform(val_test_transform)\n",
    "\n",
    "\n",
    "def get_data_transforms():\n",
    "    \"\"\"Define standard data transformations for training and validation/testing.\"\"\"\n",
    "    # Data augmentation and normalization for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(Config.img_size + 32),\n",
    "        transforms.RandomCrop(Config.img_size),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(Config.normalize_mean, Config.normalize_std),\n",
    "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "    ])\n",
    "\n",
    "    # Just normalization for validation & testing\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize(Config.img_size + 32),\n",
    "        transforms.CenterCrop(Config.img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(Config.normalize_mean, Config.normalize_std),\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_test_transform\n",
    "\n",
    "\n",
    "def compute_class_weights(dataset):\n",
    "    \"\"\"Compute class weights for imbalanced datasets\"\"\"\n",
    "    # Count samples per class\n",
    "    class_counts = defaultdict(int)\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    # Convert to lists\n",
    "    labels = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    \n",
    "    # Compute weights\n",
    "    total = sum(counts)\n",
    "    weights = [total / (len(labels) * count) for count in counts]\n",
    "    \n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "\n",
    "def create_weighted_sampler(dataset):\n",
    "    \"\"\"Create weighted sampler for imbalanced datasets\"\"\"\n",
    "    # Count samples per class\n",
    "    class_counts = defaultdict(int)\n",
    "    labels = []\n",
    "    \n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Calculate weights\n",
    "    total_samples = len(dataset)\n",
    "    num_classes = len(class_counts)\n",
    "    \n",
    "    class_weights = {}\n",
    "    for class_id, count in class_counts.items():\n",
    "        class_weights[class_id] = total_samples / (num_classes * count)\n",
    "    \n",
    "    # Assign weight to each sample\n",
    "    sample_weights = [class_weights[label] for label in labels]\n",
    "    \n",
    "    return WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_dataset_distribution(dataset, class_names):\n",
    "    \"\"\"Analyze and visualize dataset class distribution\"\"\"\n",
    "    class_counts = defaultdict(int)\n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    classes = [class_names[i] for i in sorted(class_counts.keys())]\n",
    "    counts = [class_counts[i] for i in sorted(class_counts.keys())]\n",
    "    \n",
    "    plt.bar(range(len(classes)), counts)\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title('Dataset Class Distribution')\n",
    "    plt.xticks(range(len(classes)), classes, rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.results_dir, 'class_distribution.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    total = sum(counts)\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Number of classes: {len(classes)}\")\n",
    "    print(f\"Average samples per class: {total/len(classes):.1f}\")\n",
    "    print(f\"Min samples: {min(counts)} ({classes[counts.index(min(counts))]})\")\n",
    "    print(f\"Max samples: {max(counts)} ({classes[counts.index(max(counts))]})\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def load_and_split_dataset():\n",
    "    \"\"\"Load the dataset and split it into train, validation and test sets.\"\"\"\n",
    "    # Get transforms\n",
    "    if Config.use_advanced_augmentation and ALBUMENTATIONS_AVAILABLE:\n",
    "        train_transform, val_test_transform = get_advanced_transforms()\n",
    "    else:\n",
    "        train_transform, val_test_transform = get_data_transforms()\n",
    "\n",
    "    # Load the full dataset with training transformations\n",
    "    full_dataset = datasets.ImageFolder(root=Config.data_dir, transform=train_transform)\n",
    "\n",
    "    # Create a dataset with validation/test transformations\n",
    "    val_test_dataset = datasets.ImageFolder(root=Config.data_dir, transform=val_test_transform)\n",
    "\n",
    "    # Get class names and count\n",
    "    class_names = full_dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    print(f\"Found {num_classes} classes: {class_names[:5]}{'...' if len(class_names) > 5 else ''}\")\n",
    "\n",
    "    # Calculate splits\n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(Config.train_ratio * dataset_size)\n",
    "    val_size = int(Config.val_ratio * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    print(f\"Total dataset size: {dataset_size}\")\n",
    "    print(f\"Training set size: {train_size}\")\n",
    "    print(f\"Validation set size: {val_size}\")\n",
    "    print(f\"Testing set size: {test_size}\")\n",
    "\n",
    "    # Create the splits\n",
    "    train_dataset, val_dataset_with_aug, test_dataset_with_aug = random_split(\n",
    "        full_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    # Create validation and test datasets with proper transforms\n",
    "    _, val_dataset_proper, test_dataset_proper = random_split(\n",
    "        val_test_dataset,\n",
    "        [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    # Analyze dataset distribution\n",
    "    print(\"\\nAnalyzing dataset distribution...\")\n",
    "    analyze_dataset_distribution(train_dataset, class_names)\n",
    "\n",
    "    return (\n",
    "        train_dataset,\n",
    "        val_dataset_proper,\n",
    "        test_dataset_proper,\n",
    "        class_names,\n",
    "        num_classes,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset):\n",
    "    \"\"\"Create DataLoader objects for train, validation, and test datasets.\"\"\"\n",
    "    \n",
    "    # Create weighted sampler for training if enabled\n",
    "    train_sampler = None\n",
    "    shuffle_train = True\n",
    "    \n",
    "    if Config.use_weighted_sampling:\n",
    "        train_sampler = create_weighted_sampler(train_dataset)\n",
    "        shuffle_train = False  # Cannot use shuffle with sampler\n",
    "        print(\"Using weighted sampling for imbalanced dataset\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=shuffle_train,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=Config.pin_memory,\n",
    "        drop_last=True,  # For batch norm stability\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=Config.pin_memory,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=Config.pin_memory,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Enhanced Model Architectures =====\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Enhanced Custom CNN architecture with residual connections\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.1),\n",
    "        )\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.2),\n",
    "        )\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(0.3),\n",
    "        )\n",
    "\n",
    "        # Classifier with attention\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 512),  # 256 * 2 (avg + max pooling)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        avg_pool = self.global_avg_pool(x)\n",
    "        max_pool = self.global_max_pool(x)\n",
    "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def get_model(model_type, num_classes, pretrained=True):\n",
    "    \"\"\"Create a model based on the specified type with enhanced options.\"\"\"\n",
    "    print(f\"Creating {model_type} model...\")\n",
    "\n",
    "    if model_type == \"custom_cnn\":\n",
    "        model = CustomCNN(num_classes)\n",
    "\n",
    "    elif model_type == \"resnet18\":\n",
    "        if pretrained:\n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            model = models.resnet18(weights=None)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "\n",
    "    elif model_type == \"resnet50\":\n",
    "        if pretrained:\n",
    "            model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        else:\n",
    "            model = models.resnet50(weights=None)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "\n",
    "    elif model_type == \"resnet101\":\n",
    "        if pretrained:\n",
    "            model = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2)\n",
    "        else:\n",
    "            model = models.resnet101(weights=None)\n",
    "        num_features = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "\n",
    "    elif model_type == \"efficientnet_b0\":\n",
    "        if pretrained:\n",
    "            model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            model = models.efficientnet_b0(weights=None)\n",
    "        num_features = model.classifier[1].in_features\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "\n",
    "    elif model_type == \"vit_b_16\":\n",
    "        if pretrained:\n",
    "            model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            model = models.vit_b_16(weights=None)\n",
    "        num_features = model.heads.head.in_features\n",
    "        model.heads.head = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Model type {model_type} not supported\")\n",
    "\n",
    "    # Freeze backbone if specified\n",
    "    if Config.freeze_backbone and pretrained and model_type != \"custom_cnn\":\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze classifier\n",
    "        if hasattr(model, 'fc'):\n",
    "            for param in model.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif hasattr(model, 'classifier'):\n",
    "            for param in model.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif hasattr(model, 'heads'):\n",
    "            for param in model.heads.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def unfreeze_backbone(model, model_type):\n",
    "    \"\"\"Unfreeze the backbone of a pretrained model\"\"\"\n",
    "    if model_type == \"custom_cnn\":\n",
    "        return  # Nothing to unfreeze\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    print(\"Backbone unfrozen - all parameters are now trainable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ae733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Enhanced Training Functions =====\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Handle model checkpointing and saving best models\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir, monitor='val_acc', save_top_k=3):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.monitor = monitor\n",
    "        self.save_top_k = save_top_k\n",
    "        self.best_scores = []\n",
    "        self.best_paths = []\n",
    "        \n",
    "    def save_checkpoint(self, model, optimizer, scheduler, epoch, metrics, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "            'config': vars(Config)\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pth'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = self.checkpoint_dir / 'best_model.pth'\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"New best model saved: {self.monitor} = {metrics[self.monitor]:.4f}\")\n",
    "            \n",
    "            # Manage top-k models\n",
    "            self._manage_top_k_models(checkpoint_path, metrics[self.monitor])\n",
    "        \n",
    "        return checkpoint_path\n",
    "    \n",
    "    def _manage_top_k_models(self, checkpoint_path, score):\n",
    "        \"\"\"Keep only top-k best models\"\"\"\n",
    "        self.best_scores.append(score)\n",
    "        self.best_paths.append(checkpoint_path)\n",
    "        \n",
    "        # Sort by score (descending for accuracy, ascending for loss)\n",
    "        is_accuracy = 'acc' in self.monitor.lower()\n",
    "        sorted_pairs = sorted(zip(self.best_scores, self.best_paths), \n",
    "                            key=lambda x: x[0], reverse=is_accuracy)\n",
    "        \n",
    "        # Keep only top-k\n",
    "        if len(sorted_pairs) > self.save_top_k:\n",
    "            to_remove = sorted_pairs[self.save_top_k:]\n",
    "            for _, path in to_remove:\n",
    "                if path.exists():\n",
    "                    path.unlink()\n",
    "            \n",
    "            # Update lists\n",
    "            sorted_pairs = sorted_pairs[:self.save_top_k]\n",
    "            self.best_scores, self.best_paths = zip(*sorted_pairs)\n",
    "            self.best_scores = list(self.best_scores)\n",
    "            self.best_paths = list(self.best_paths)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=Config.device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint.get('metrics', {})\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss\"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        n_classes = pred.size(-1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - self.smoothing) + (1 - one_hot) * self.smoothing / (n_classes - 1)\n",
    "        log_prob = F.log_softmax(pred, dim=-1)\n",
    "        return -(one_hot * log_prob).sum(dim=-1).mean()\n",
    "\n",
    "\n",
    "def create_optimizer_and_scheduler(model, train_loader):\n",
    "    \"\"\"Create optimizer and learning rate scheduler\"\"\"\n",
    "    # Different learning rates for different parts of the model\n",
    "    if Config.model_type != \"custom_cnn\" and Config.pretrained:\n",
    "        # Separate parameters for backbone and classifier\n",
    "        backbone_params = []\n",
    "        classifier_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'fc' in name or 'classifier' in name or 'heads' in name:\n",
    "                classifier_params.append(param)\n",
    "            else:\n",
    "                backbone_params.append(param)\n",
    "        \n",
    "        # Use lower learning rate for backbone\n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': backbone_params, 'lr': Config.learning_rate * 0.1},\n",
    "            {'params': classifier_params, 'lr': Config.learning_rate}\n",
    "        ], weight_decay=Config.weight_decay)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=Config.learning_rate,\n",
    "            weight_decay=Config.weight_decay\n",
    "        )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    if Config.use_cosine_annealing:\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=len(train_loader) * 10,  # Restart every 10 epochs\n",
    "            T_mult=2,\n",
    "            eta_min=Config.learning_rate * 0.001\n",
    "        )\n",
    "    else:\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "def train_model(model, dataloaders, num_epochs=None):\n",
    "    \"\"\"Enhanced training with checkpointing and advanced features\"\"\"\n",
    "    if num_epochs is None:\n",
    "        num_epochs = Config.num_epochs\n",
    "        \n",
    "    since = time.time()\n",
    "    device = Config.device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize tracking\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_acc = 0.0\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Create optimizer and scheduler\n",
    "    optimizer, scheduler = create_optimizer_and_scheduler(model, dataloaders['train'])\n",
    "\n",
    "    # Loss function with label smoothing\n",
    "    if Config.label_smoothing > 0:\n",
    "        criterion = LabelSmoothingCrossEntropy(smoothing=Config.label_smoothing)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Mixed precision scaler\n",
    "    scaler = GradScaler() if Config.use_mixed_precision else None\n",
    "\n",
    "    # Checkpointing\n",
    "    checkpoint_manager = ModelCheckpoint(\n",
    "        Config.checkpoint_dir,\n",
    "        monitor=Config.monitor_metric,\n",
    "        save_top_k=Config.save_top_k\n",
    "    )\n",
    "\n",
    "    # TensorBoard logging\n",
    "    writer = None\n",
    "    if Config.use_tensorboard:\n",
    "        writer = SummaryWriter(Config.tensorboard_dir)\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    if Config.resume_from_checkpoint:\n",
    "        print(f\"Resuming from checkpoint: {Config.resume_from_checkpoint}\")\n",
    "        start_epoch, metrics = load_checkpoint(\n",
    "            Config.resume_from_checkpoint, model, optimizer, scheduler\n",
    "        )\n",
    "        best_acc = metrics.get('val_acc', 0.0)\n",
    "        start_epoch += 1\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {total_params:,} parameters, {trainable_params:,} are trainable\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stop_counter = 0\n",
    "    epochs_since_improvement = 0\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, num_epochs):\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "            print('-' * 50)\n",
    "\n",
    "            # Unfreeze backbone if specified\n",
    "            if Config.freeze_backbone and epoch == Config.unfreeze_epoch:\n",
    "                unfreeze_backbone(model, Config.model_type)\n",
    "                # Recreate optimizer with all parameters\n",
    "                optimizer, scheduler = create_optimizer_and_scheduler(model, dataloaders['train'])\n",
    "\n",
    "            epoch_metrics = {}\n",
    "\n",
    "            # Training and validation phases\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "                num_samples = 0\n",
    "\n",
    "                # Progress bar\n",
    "                pbar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()}')\n",
    "\n",
    "                for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "                    inputs = inputs.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass with mixed precision\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        if Config.use_mixed_precision and phase == 'train':\n",
    "                            with autocast():\n",
    "                                outputs = model(inputs)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            scaler.scale(loss).backward()\n",
    "                            \n",
    "                            # Gradient clipping\n",
    "                            if Config.gradient_clip_val > 0:\n",
    "                                scaler.unscale_(optimizer)\n",
    "                                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.gradient_clip_val)\n",
    "                            \n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            if phase == 'train':\n",
    "                                loss.backward()\n",
    "                                \n",
    "                                # Gradient clipping\n",
    "                                if Config.gradient_clip_val > 0:\n",
    "                                    torch.nn.utils.clip_grad_norm_(model.parameters(), Config.gradient_clip_val)\n",
    "                                \n",
    "                                optimizer.step()\n",
    "\n",
    "                    # Statistics\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                    num_samples += inputs.size(0)\n",
    "\n",
    "                    # Update progress bar\n",
    "                    current_acc = running_corrects.double() / num_samples\n",
    "                    pbar.set_postfix({\n",
    "                        'Loss': f'{running_loss/num_samples:.4f}',\n",
    "                        'Acc': f'{current_acc:.4f}'\n",
    "                    })\n",
    "\n",
    "                    # Update scheduler for cosine annealing\n",
    "                    if Config.use_cosine_annealing and phase == 'train' and isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "                        scheduler.step(epoch + batch_idx / len(dataloaders[phase]))\n",
    "\n",
    "                # Epoch statistics\n",
    "                epoch_loss = running_loss / num_samples\n",
    "                epoch_acc = running_corrects.double() / num_samples\n",
    "\n",
    "                print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # Store metrics\n",
    "                epoch_metrics[f'{phase}_loss'] = epoch_loss\n",
    "                epoch_metrics[f'{phase}_acc'] = epoch_acc.item()\n",
    "                history[f'{phase}_loss'].append(epoch_loss)\n",
    "                history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "\n",
    "                # Log to TensorBoard\n",
    "                if writer:\n",
    "                    writer.add_scalar(f'{phase}/Loss', epoch_loss, epoch)\n",
    "                    writer.add_scalar(f'{phase}/Accuracy', epoch_acc, epoch)\n",
    "\n",
    "                # Update scheduler for ReduceLROnPlateau\n",
    "                if phase == 'val' and not Config.use_cosine_annealing:\n",
    "                    scheduler.step(epoch_acc)\n",
    "\n",
    "            # Check for improvement\n",
    "            current_val_acc = epoch_metrics['val_acc']\n",
    "            is_best = current_val_acc > best_acc\n",
    "            \n",
    "            if is_best:\n",
    "                best_acc = current_val_acc\n",
    "                epochs_since_improvement = 0\n",
    "            else:\n",
    "                epochs_since_improvement += 1\n",
    "\n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % Config.checkpoint_every == 0 or is_best:\n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    model, optimizer, scheduler, epoch, epoch_metrics, is_best\n",
    "                )\n",
    "\n",
    "            # Log to wandb\n",
    "            if Config.use_wandb:\n",
    "                wandb.log(epoch_metrics, step=epoch)\n",
    "\n",
    "            # Early stopping\n",
    "            if epochs_since_improvement >= Config.patience:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs!')\n",
    "                break\n",
    "\n",
    "            print(f'Best val Acc so far: {best_acc:.4f}')\n",
    "            print()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Training interrupted by user')\n",
    "\n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.close()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model\n",
    "    best_model_path = Config.checkpoint_dir + '/best_model.pth'\n",
    "    if os.path.exists(best_model_path):\n",
    "        load_checkpoint(best_model_path, model)\n",
    "        print('Loaded best model weights')\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Enhanced Evaluation Functions =====\n",
    "def evaluate_model(model, test_loader, class_names, save_results=True):\n",
    "    \"\"\"Comprehensive model evaluation with detailed metrics\"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(Config.device)\n",
    "\n",
    "    # Lists to store predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # No gradient calculation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Evaluating'):\n",
    "            inputs = inputs.to(Config.device)\n",
    "            labels = labels.to(Config.device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate comprehensive metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Detailed classification report\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds, target_names=class_names, digits=4, output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None\n",
    "    )\n",
    "\n",
    "    # Macro and weighted averages\n",
    "    macro_f1 = precision_recall_fscore_support(all_labels, all_preds, average='macro')[2]\n",
    "    weighted_f1 = precision_recall_fscore_support(all_labels, all_preds, average='weighted')[2]\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n",
    "    print(\"\\nPer-class metrics:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name:30} - Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1: {f1[i]:.4f}\")\n",
    "\n",
    "    # Save detailed results\n",
    "    if save_results:\n",
    "        results = {\n",
    "            'accuracy': accuracy,\n",
    "            'macro_f1': macro_f1,\n",
    "            'weighted_f1': weighted_f1,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix.tolist(),\n",
    "            'class_names': class_names\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(Config.results_dir, 'evaluation_results.json'), 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    return accuracy, conf_matrix, report, all_probs\n",
    "\n",
    "\n",
    "def compute_class_performance(all_labels, all_preds, class_names):\n",
    "    \"\"\"Compute detailed per-class performance metrics\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        # Binary classification metrics for each class\n",
    "        y_true_binary = [1 if label == i else 0 for label in all_labels]\n",
    "        y_pred_binary = [1 if pred == i else 0 for pred in all_preds]\n",
    "        \n",
    "        precision = precision_recall_fscore_support(y_true_binary, y_pred_binary, average='binary')[0]\n",
    "        recall = precision_recall_fscore_support(y_true_binary, y_pred_binary, average='binary')[1]\n",
    "        f1 = precision_recall_fscore_support(y_true_binary, y_pred_binary, average='binary')[2]\n",
    "        \n",
    "        results[class_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'support': sum(y_true_binary)\n",
    "        }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Enhanced Visualization Functions =====\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot comprehensive training history with subplots\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    axes[0, 0].plot(history[\"train_loss\"], label=\"Training Loss\", linewidth=2)\n",
    "    axes[0, 0].plot(history[\"val_loss\"], label=\"Validation Loss\", linewidth=2)\n",
    "    axes[0, 0].set_title(\"Training and Validation Loss\", fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel(\"Epochs\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot training & validation accuracy\n",
    "    axes[0, 1].plot(history[\"train_acc\"], label=\"Training Accuracy\", linewidth=2)\n",
    "    axes[0, 1].plot(history[\"val_acc\"], label=\"Validation Accuracy\", linewidth=2)\n",
    "    axes[0, 1].set_title(\"Training and Validation Accuracy\", fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel(\"Epochs\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot learning rate (if available)\n",
    "    if len(history.get('lr', [])) > 0:\n",
    "        axes[1, 0].plot(history[\"lr\"], label=\"Learning Rate\", linewidth=2, color='orange')\n",
    "        axes[1, 0].set_title(\"Learning Rate Schedule\", fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel(\"Epochs\")\n",
    "        axes[1, 0].set_ylabel(\"Learning Rate\")\n",
    "        axes[1, 0].set_yscale('log')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Learning Rate\\nNot Tracked', ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "    # Plot validation accuracy zoomed\n",
    "    if len(history[\"val_acc\"]) > 0:\n",
    "        axes[1, 1].plot(history[\"val_acc\"], label=\"Validation Accuracy\", linewidth=2, color='green')\n",
    "        axes[1, 1].set_title(\"Validation Accuracy (Detailed)\", fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel(\"Epochs\")\n",
    "        axes[1, 1].set_ylabel(\"Accuracy\")\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add best accuracy annotation\n",
    "        best_acc = max(history[\"val_acc\"])\n",
    "        best_epoch = history[\"val_acc\"].index(best_acc)\n",
    "        axes[1, 1].annotate(f'Best: {best_acc:.4f}', \n",
    "                           xy=(best_epoch, best_acc), \n",
    "                           xytext=(best_epoch + 2, best_acc - 0.02),\n",
    "                           arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Training history plot saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, class_names, save_path=None, figsize=(12, 10)):\n",
    "    \"\"\"Plot enhanced confusion matrix with better formatting\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Normalize the confusion matrix\n",
    "    norm_conf_matrix = conf_matrix.astype(\"float\") / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Create heatmap\n",
    "    mask = conf_matrix == 0\n",
    "    sns.heatmap(\n",
    "        norm_conf_matrix,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\".3f\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar_kws={'label': 'Normalized Count'},\n",
    "        mask=mask\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Normalized Confusion Matrix\", fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylabel(\"True Label\", fontsize=12)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(class_counts, class_names, save_path=None):\n",
    "    \"\"\"Plot class distribution with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Sort classes by count for better visualization\n",
    "    sorted_data = sorted(zip(class_names, class_counts), key=lambda x: x[1], reverse=True)\n",
    "    sorted_names, sorted_counts = zip(*sorted_data)\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = plt.bar(range(len(sorted_names)), sorted_counts, \n",
    "                   color=plt.cm.viridis(np.linspace(0, 1, len(sorted_names))))\n",
    "    \n",
    "    plt.xlabel('Classes', fontsize=12)\n",
    "    plt.ylabel('Number of Samples', fontsize=12)\n",
    "    plt.title('Dataset Class Distribution (Sorted by Count)', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(range(len(sorted_names)), sorted_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, sorted_counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sorted_counts)*0.01,\n",
    "                str(count), ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Class distribution plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_model_predictions(model, dataset, class_names, num_samples=8, save_path=None):\n",
    "    \"\"\"Visualize model predictions with confidence scores\"\"\"\n",
    "    model.eval()\n",
    "    device = Config.device\n",
    "    \n",
    "    # Get random samples\n",
    "    indices = torch.randperm(len(dataset))[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Get image and true label\n",
    "        image, true_label = dataset[idx]\n",
    "        \n",
    "        # Prepare image for display\n",
    "        img_display = image.clone()\n",
    "        if img_display.shape[0] == 3:  # If channels first\n",
    "            img_display = img_display.permute(1, 2, 0)\n",
    "        \n",
    "        # Denormalize for display\n",
    "        mean = torch.tensor(Config.normalize_mean)\n",
    "        std = torch.tensor(Config.normalize_std)\n",
    "        img_display = img_display * std + mean\n",
    "        img_display = torch.clamp(img_display, 0, 1)\n",
    "        \n",
    "        # Make prediction\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(image_input)\n",
    "            probabilities = F.softmax(output, dim=1)\n",
    "            confidence, predicted = torch.max(probabilities, 1)\n",
    "        \n",
    "        # Get class names\n",
    "        true_class = class_names[true_label]\n",
    "        pred_class = class_names[predicted.item()]\n",
    "        conf_score = confidence.item()\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Color code: green for correct, red for incorrect\n",
    "        color = 'green' if predicted.item() == true_label else 'red'\n",
    "        title = f'True: {true_class}\\nPred: {pred_class}\\nConf: {conf_score:.3f}'\n",
    "        axes[i].set_title(title, color=color, fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(indices), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions with Confidence Scores', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Prediction visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e586f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Prediction Demo =====\n",
    "def predict_random_images(model, test_dataset, class_names, num_images=5):\n",
    "    \"\"\"Display and predict random images from the test set.\"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(Config.device)\n",
    "\n",
    "    # Get a batch of random indices\n",
    "    indices = torch.randperm(len(test_dataset))[:num_images]\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(15, 3 * num_images))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Get image and label\n",
    "        image, label = test_dataset[idx]\n",
    "\n",
    "        # Convert image for display\n",
    "        image_for_display = image.clone()\n",
    "\n",
    "        # Make prediction\n",
    "        image = image.unsqueeze(0).to(Config.device)\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "\n",
    "        # Get predicted and true class names\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "        true_class = class_names[label]\n",
    "\n",
    "        # Display the image\n",
    "        plt.subplot(num_images, 1, i + 1)\n",
    "        plt.imshow(np.transpose(image_for_display.cpu().numpy(), (1, 2, 0)))\n",
    "\n",
    "        # Normalize the image for better display\n",
    "        plt.title(f\"True: {true_class} | Predicted: {predicted_class}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Color based on correctness\n",
    "        if predicted.item() == label:\n",
    "            plt.title(\n",
    "                f\"True: {true_class} | Predicted: {predicted_class}\", color=\"green\"\n",
    "            )\n",
    "        else:\n",
    "            plt.title(f\"True: {true_class} | Predicted: {predicted_class}\", color=\"red\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.results_dir, \"sample_predictions.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d14364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Section: Enhanced Main Execution =====\n",
    "def main():\n",
    "    \"\"\"Enhanced main execution function with comprehensive training pipeline\"\"\"\n",
    "    print(\" Starting Enhanced Crop Disease Classification Training Pipeline\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Print configuration\n",
    "    print(f\"\\n Configuration:\")\n",
    "    print(f\"   Model: {Config.model_type}\")\n",
    "    print(f\"   Pretrained: {Config.pretrained}\")\n",
    "    print(f\"   Image size: {Config.img_size}x{Config.img_size}\")\n",
    "    print(f\"   Batch size: {Config.batch_size}\")\n",
    "    print(f\"   Learning rate: {Config.learning_rate}\")\n",
    "    print(f\"   Epochs: {Config.num_epochs}\")\n",
    "    print(f\"   Device: {Config.device}\")\n",
    "    print(f\"   Mixed precision: {Config.use_mixed_precision}\")\n",
    "    print(f\"   Advanced augmentation: {Config.use_advanced_augmentation}\")\n",
    "\n",
    "    # 1. Load and prepare data\n",
    "    print(\"\\n Preparing datasets...\")\n",
    "    train_dataset, val_dataset, test_dataset, class_names, num_classes = load_and_split_dataset()\n",
    "\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_dataset, val_dataset, test_dataset\n",
    "    )\n",
    "    dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "    # 2. Create model\n",
    "    print(f\"\\n  Creating {Config.model_type} model...\")\n",
    "    model = get_model(Config.model_type, num_classes, Config.pretrained)\n",
    "    model = model.to(Config.device)\n",
    "\n",
    "    # Print model summary\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Model size: {total_params * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "    # 3. Train the model\n",
    "    print(f\"\\n  Training {Config.model_type} model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model, history = train_model(model, dataloaders, Config.num_epochs)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n  Training completed in {training_time//3600:.0f}h {(training_time%3600)//60:.0f}m {training_time%60:.0f}s\")\n",
    "\n",
    "    # 4. Evaluate on test set\n",
    "    print(\"\\n Evaluating on test set...\")\n",
    "    accuracy, conf_matrix, report, all_probs = evaluate_model(model, test_loader, class_names)\n",
    "\n",
    "    # 5. Create comprehensive visualizations\n",
    "    print(\"\\n Creating visualizations...\")\n",
    "    \n",
    "    # Training history\n",
    "    plot_training_history(\n",
    "        history,\n",
    "        save_path=os.path.join(Config.results_dir, f\"{Config.model_type}_training_history.png\")\n",
    "    )\n",
    "\n",
    "    # Confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        conf_matrix,\n",
    "        class_names,\n",
    "        save_path=os.path.join(Config.results_dir, f\"{Config.model_type}_confusion_matrix.png\")\n",
    "    )\n",
    "\n",
    "    # Model predictions visualization\n",
    "    visualize_model_predictions(\n",
    "        model, test_dataset, class_names,\n",
    "        save_path=os.path.join(Config.results_dir, f\"{Config.model_type}_predictions.png\")\n",
    "    )\n",
    "\n",
    "    # 6. Save models and results\n",
    "    print(\"\\n Saving results...\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(Config.output_dir, f\"{Config.model_type}_final.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'class_names': class_names,\n",
    "        'num_classes': num_classes,\n",
    "        'config': vars(Config),\n",
    "        'accuracy': accuracy,\n",
    "        'training_history': history\n",
    "    }, final_model_path)\n",
    "    print(f\"   Final model saved to {final_model_path}\")\n",
    "\n",
    "    # Save best model for production use\n",
    "    production_model_path = os.path.join(Config.output_dir, f\"{Config.model_type}_production.pth\")\n",
    "    torch.save(model.state_dict(), production_model_path)\n",
    "    print(f\"   Production model saved to {production_model_path}\")\n",
    "\n",
    "    # 7. Generate comprehensive report\n",
    "    print(\"\\n Generating comprehensive report...\")\n",
    "    \n",
    "    # Training summary\n",
    "    best_val_acc = max(history['val_acc']) if history['val_acc'] else 0\n",
    "    best_epoch = history['val_acc'].index(best_val_acc) + 1 if history['val_acc'] else 0\n",
    "    \n",
    "    report_content = f\"\"\"#  Crop Disease Classification - Training Report\n",
    "\n",
    "##  Experiment Configuration\n",
    "- **Model Architecture**: {Config.model_type}\n",
    "- **Pretrained**: {Config.pretrained}\n",
    "- **Image Size**: {Config.img_size}x{Config.img_size}\n",
    "- **Batch Size**: {Config.batch_size}\n",
    "- **Learning Rate**: {Config.learning_rate}\n",
    "- **Weight Decay**: {Config.weight_decay}\n",
    "- **Epochs Trained**: {len(history['train_loss']) if history['train_loss'] else 0}\n",
    "- **Early Stopping Patience**: {Config.patience}\n",
    "- **Mixed Precision**: {Config.use_mixed_precision}\n",
    "- **Advanced Augmentation**: {Config.use_advanced_augmentation}\n",
    "- **Augmentation Strength**: {Config.augmentation_strength}\n",
    "\n",
    "##  Dataset Information\n",
    "- **Total Classes**: {num_classes}\n",
    "- **Training Samples**: {len(train_dataset)}\n",
    "- **Validation Samples**: {len(val_dataset)}\n",
    "- **Test Samples**: {len(test_dataset)}\n",
    "\n",
    "##  Performance Results\n",
    "- **Best Validation Accuracy**: {best_val_acc:.4f} (Epoch {best_epoch})\n",
    "- **Final Test Accuracy**: {accuracy:.4f}\n",
    "- **Training Time**: {training_time//3600:.0f}h {(training_time%3600)//60:.0f}m {training_time%60:.0f}s\n",
    "\n",
    "##  Model Information\n",
    "- **Total Parameters**: {total_params:,}\n",
    "- **Trainable Parameters**: {trainable_params:,}\n",
    "- **Model Size**: {total_params * 4 / 1024**2:.2f} MB\n",
    "\n",
    "##  Training Progress\n",
    "- **Final Training Loss**: {history['train_loss'][-1]:.4f if history['train_loss'] else 'N/A'}\n",
    "- **Final Validation Loss**: {history['val_loss'][-1]:.4f if history['val_loss'] else 'N/A'}\n",
    "- **Final Training Accuracy**: {history['train_acc'][-1]:.4f if history['train_acc'] else 'N/A'}\n",
    "- **Final Validation Accuracy**: {history['val_acc'][-1]:.4f if history['val_acc'] else 'N/A'}\n",
    "\n",
    "##  Detailed Classification Results\n",
    "```\n",
    "{classification_report(list(range(len(class_names))), list(range(len(class_names))), target_names=class_names) if class_names else 'N/A'}\n",
    "```\n",
    "\n",
    "##  Generated Files\n",
    "- Model checkpoint: `{final_model_path}`\n",
    "- Production model: `{production_model_path}`\n",
    "- Training history: `training_history.json`\n",
    "- Evaluation results: `evaluation_results.json`\n",
    "- Visualizations: `{Config.results_dir}/`\n",
    "\n",
    "##  Reproduction Commands\n",
    "```bash\n",
    "# Install dependencies\n",
    "pip install torch torchvision albumentations wandb tensorboard\n",
    "\n",
    "# Run training\n",
    "python model_training.py\n",
    "```\n",
    "\n",
    "---\n",
    "*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\"\"\"\n",
    "\n",
    "    # Save report\n",
    "    report_path = os.path.join(Config.results_dir, \"training_report.md\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report_content)\n",
    "    print(f\"   Comprehensive report saved to {report_path}\")\n",
    "\n",
    "    # Save training history as JSON\n",
    "    history_path = os.path.join(Config.results_dir, \"training_history.json\")\n",
    "    history_dict = {\n",
    "        \"train_loss\": [float(val) for val in history[\"train_loss\"]],\n",
    "        \"val_loss\": [float(val) for val in history[\"val_loss\"]],\n",
    "        \"train_acc\": [float(val) for val in history[\"train_acc\"]],\n",
    "        \"val_acc\": [float(val) for val in history[\"val_acc\"]],\n",
    "        \"config\": vars(Config),\n",
    "        \"class_names\": class_names,\n",
    "        \"num_classes\": num_classes\n",
    "    }\n",
    "    \n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history_dict, f, indent=4)\n",
    "    print(f\"   Training history saved to {history_path}\")\n",
    "\n",
    "    # 8. Final summary\n",
    "    print(\"\\n Training Pipeline Completed Successfully!\")\n",
    "    print(f\"    Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"    Final test accuracy: {accuracy:.4f}\")\n",
    "    print(f\"    Models saved in: {Config.output_dir}\")\n",
    "    print(f\"    Results saved in: {Config.results_dir}\")\n",
    "    \n",
    "    if Config.use_tensorboard:\n",
    "        print(f\"    TensorBoard logs: {Config.tensorboard_dir}\")\n",
    "        print(f\"    View with: tensorboard --logdir {Config.tensorboard_dir}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "    return model, history, accuracy\n",
    "\n",
    "\n",
    "# ===== Utility Functions =====\n",
    "def resume_training(checkpoint_path, data_dir=None):\n",
    "    \"\"\"Resume training from a checkpoint\"\"\"\n",
    "    if data_dir:\n",
    "        Config.data_dir = data_dir\n",
    "    \n",
    "    Config.resume_from_checkpoint = checkpoint_path\n",
    "    print(f\"Resuming training from: {checkpoint_path}\")\n",
    "    \n",
    "    return main()\n",
    "\n",
    "\n",
    "def evaluate_saved_model(model_path, data_dir=None):\n",
    "    \"\"\"Evaluate a saved model\"\"\"\n",
    "    if data_dir:\n",
    "        Config.data_dir = data_dir\n",
    "    \n",
    "    # Load data\n",
    "    _, _, test_dataset, class_names, num_classes = load_and_split_dataset()\n",
    "    _, _, test_loader = create_dataloaders(None, None, test_dataset)\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=Config.device)\n",
    "    model = get_model(Config.model_type, num_classes, False)\n",
    "    \n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy, conf_matrix, report, _ = evaluate_model(model, test_loader, class_names)\n",
    "    return accuracy, conf_matrix, report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6e420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64508473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
